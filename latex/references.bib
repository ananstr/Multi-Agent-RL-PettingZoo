
@misc{cao_emergent_2018,
	title = {Emergent Communication through Negotiation},
	url = {http://arxiv.org/abs/1804.03980},
	doi = {10.48550/arXiv.1804.03980},
	abstract = {Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols -- one grounded in the semantics of the game, and one which is {\textbackslash}textit\{a priori\} ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.},
	number = {{arXiv}:1804.03980},
	publisher = {{arXiv}},
	author = {Cao, Kris and Lazaridou, Angeliki and Lanctot, Marc and Leibo, Joel Z. and Tuyls, Karl and Clark, Stephen},
	urldate = {2025-06-23},
	date = {2018-04-11},
	eprinttype = {arxiv},
	eprint = {1804.03980 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{eccles_biases_2019,
	title = {Biases for Emergent Communication in Multi-agent Reinforcement Learning},
	url = {http://arxiv.org/abs/1912.05676},
	doi = {10.48550/arXiv.1912.05676},
	abstract = {We study the problem of emergent communication, in which language arises because speakers and listeners must communicate information in order to solve tasks. In temporally extended reinforcement learning domains, it has proved hard to learn such communication without centralized training of agents, due in part to a difficult joint exploration problem. We introduce inductive biases for positive signalling and positive listening, which ease this problem. In a simple one-step environment, we demonstrate how these biases ease the learning problem. We also apply our methods to a more extended environment, showing that agents with these inductive biases achieve better performance, and analyse the resulting communication protocols.},
	number = {{arXiv}:1912.05676},
	publisher = {{arXiv}},
	author = {Eccles, Tom and Bachrach, Yoram and Lever, Guy and Lazaridou, Angeliki and Graepel, Thore},
	urldate = {2025-06-23},
	date = {2019-12-11},
	eprinttype = {arxiv},
	eprint = {1912.05676 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@inproceedings{foerster_learning_2016,
	location = {Red Hook, {NY}, {USA}},
	title = {Learning to communicate with Deep multi-agent reinforcement learning},
	isbn = {978-1-5108-3881-9},
	series = {{NIPS}'16},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning ({RIAL}) and Differentiable Inter-Agent Learning ({DIAL}). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	pages = {2145--2153},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
	urldate = {2025-06-23},
	date = {2016},
}

@online{lowe_learning_2017,
	title = {Learning to cooperate, compete, and communicate},
	url = {https://openai.com/index/learning-to-cooperate-compete-and-communicate/},
	abstract = {Multiagent environments where agents compete for resources are stepping stones on the path to {AGI}. Multiagent environments have two useful properties: first, there is a natural curriculum—the difficulty of the environment is determined by the skill of your competitors (and if you’re competing against clones of yourself, the environment exactly matches your skill level). Second, a multiagent environment has no stable equilibrium: no matter how smart an agent is, there’s always pressure to get smarter. These environments have a very different feel from traditional environments, and it’ll take a lot more research before we become good at them.},
	author = {Lowe, Ryan and Mordatch, Igor and Abbeel, Pieter and Wu, Yi and Tamar, Aviv and Harb, Jean},
	urldate = {2025-06-23},
	date = {2017-06-07},
	langid = {american},
}

@article{hernandez-leal_survey_2019,
	title = {A survey and critique of multiagent deep reinforcement learning},
	volume = {33},
	issn = {1573-7454},
	url = {https://doi.org/10.1007/s10458-019-09421-1},
	doi = {10.1007/s10458-019-09421-1},
	abstract = {Deep reinforcement learning ({RL}) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning ({MAL}) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning ({MDRL}) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in {MAL} and {RL}, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from {MDRL} works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of {MDRL} (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., {RL} and {MAL}) in a joint effort to promote fruitful research in the multiagent community.},
	pages = {750--797},
	number = {6},
	journaltitle = {Autonomous Agents and Multi-Agent Systems},
	shortjournal = {Auton Agent Multi-Agent Syst},
	author = {Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E.},
	urldate = {2025-06-23},
	date = {2019-11-01},
	langid = {english},
	keywords = {Agent-based Economics, Artificial Intelligence, Deep reinforcement learning, Learning algorithms, Machine Learning, Multiagent Systems, Multiagent learning, Multiagent reinforcement learning, Multiagent systems, Social Robotics, Survey},
}

@article{allahham_multi-agent_2022,
	title = {Multi-Agent Reinforcement Learning for Network Selection and Resource Allocation in Heterogeneous Multi-{RAT} Networks},
	volume = {8},
	issn = {2332-7731},
	url = {https://ieeexplore.ieee.org/document/9726129},
	doi = {10.1109/TCCN.2022.3155727},
	abstract = {The rapid production of mobile devices along with the wireless applications boom is continuing to evolve daily. This motivates the exploitation of wireless spectrum using multiple Radio Access Technologies (multi-{RAT}) and developing innovative network selection techniques to cope with such intensive demand while improving Quality of Service ({QoS}). Thus, we propose a distributed framework for dynamic network selection at the edge level, and resource allocation at the Radio Access Network ({RAN}) level, while taking into consideration diverse applications’ characteristics. In particular, our framework employs a deep Multi-Agent Reinforcement Learning ({DMARL}) algorithm, that aims to maximize the edge nodes’ quality of experience while extending the battery lifetime of the nodes and leveraging adaptive compression schemes. Indeed, our framework enables data transfer from the network’s edge nodes, with multi-{RAT} capabilities, to the cloud in a cost and energy-efficient manner, while maintaining {QoS} requirements of different supported applications. Our results depict that our solution outperforms state-of-the-art techniques of network selection in terms of energy consumption, latency, and cost.},
	pages = {1287--1300},
	number = {2},
	journaltitle = {{IEEE} Transactions on Cognitive Communications and Networking},
	author = {Allahham, Mhd Saria and Abdellatif, Alaa Awad and Mhaisen, Naram and Mohamed, Amr and Erbad, Aiman and Guizani, Mohsen},
	urldate = {2025-06-23},
	date = {2022-06},
	keywords = {5G mobile communication, Heterogeneous networks, Optimization, Quality of service, Radio access technologies, Reinforcement learning, Resource management, Task analysis, deep reinforcement learning, edge computing, multi-{RAT} architecture, wireless healthcare systems},
}

@article{ning_survey_2024,
	title = {A survey on multi-agent reinforcement learning and its application},
	volume = {3},
	issn = {2949-8554},
	url = {https://www.sciencedirect.com/science/article/pii/S2949855424000042},
	doi = {10.1016/j.jai.2024.02.003},
	abstract = {Multi-agent reinforcement learning ({MARL}) has been a rapidly evolving field. This paper presents a comprehensive survey of {MARL} and its applications. We trace the historical evolution of {MARL}, highlight its progress, and discuss related survey works. Then, we review the existing works addressing inherent challenges and those focusing on diverse applications. Some representative stochastic games, {MARL} means, spatial forms of {MARL}, and task classification are revisited. We then conduct an in-depth exploration of a variety of challenges encountered in {MARL} applications. We also address critical operational aspects, such as hyperparameter tuning and computational complexity, which are pivotal in practical implementations of {MARL}. Afterward, we make a thorough overview of the applications of {MARL} to intelligent machines and devices, chemical engineering, biotechnology, healthcare, and societal issues, which highlights the extensive potential and relevance of {MARL} within both current and future technological contexts. Our survey also encompasses a detailed examination of benchmark environments used in {MARL} research, which are instrumental in evaluating {MARL} algorithms and demonstrate the adaptability of {MARL} to diverse application scenarios. In the end, we give our prospect for {MARL} and discuss their related techniques and potential future applications.},
	pages = {73--91},
	number = {2},
	journaltitle = {Journal of Automation and Intelligence},
	shortjournal = {Journal of Automation and Intelligence},
	author = {Ning, Zepeng and Xie, Lihua},
	urldate = {2025-06-23},
	date = {2024-06-01},
	keywords = {Benchmark environments, Multi-agent reinforcement learning, Multi-agent systems, Stochastic games},
}

@inproceedings{foerster_counterfactual_2018,
	location = {New Orleans, Louisiana, {USA}},
	title = {Counterfactual multi-agent policy gradients},
	isbn = {978-1-57735-800-8},
	series = {{AAAI}'18/{IAAI}'18/{EAAI}'18},
	abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent ({COMA}) policy gradients. {COMA} uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. {COMA} also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate {COMA} in the testbed of {StarCraft} unit micromanagement, using a decentralised variant with significant partial observability. {COMA} significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
	pages = {2974--2982},
	booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth {AAAI} Symposium on Educational Advances in Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Foerster, Jakob N. and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
	urldate = {2025-06-23},
	date = {2018-02-02},
}

@inproceedings{yu_surprising_2022,
	location = {Red Hook, {NY}, {USA}},
	title = {The surprising effectiveness of {PPO} in cooperative multi-agent games},
	isbn = {978-1-7138-7108-8},
	series = {{NIPS} '22},
	abstract = {Proximal Policy Optimization ({PPO}) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that {PPO} is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of {PPO} in cooperative multi-agent settings. We show that {PPO}-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the {StarCraft} multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, {PPO} often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to {PPO}'s empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple {PPO}-based methods can be a strong baseline in cooperative multi-agent reinforcement learning.},
	pages = {24611--24624},
	booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
	urldate = {2025-06-23},
	date = {2022-11-28},
}

@misc{wang_qplex_2021,
	title = {{QPLEX}: Duplex Dueling Multi-Agent Q-Learning},
	url = {http://arxiv.org/abs/2008.01062},
	doi = {10.48550/arXiv.2008.01062},
	shorttitle = {{QPLEX}},
	abstract = {We explore value-based multi-agent reinforcement learning ({MARL}) in the popular paradigm of centralized training with decentralized execution ({CTDE}). {CTDE} has an important concept, Individual-Global-Max ({IGM}) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing {MARL} methods either limit representation expressiveness of their value function classes or relax the {IGM} consistency, which may suffer from instability risk or may not perform well in complex domains. This paper presents a novel {MARL} approach, called {duPLEX} dueling multi-agent Q-learning ({QPLEX}), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the {IGM} principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that {QPLEX} achieves a complete {IGM} function class. Empirical experiments on {StarCraft} {II} micromanagement tasks demonstrate that {QPLEX} significantly outperforms state-of-the-art baselines in both online and offline data collection settings, and also reveal that {QPLEX} achieves high sample efficiency and can benefit from offline datasets without additional online exploration.},
	number = {{arXiv}:2008.01062},
	publisher = {{arXiv}},
	author = {Wang, Jianhao and Ren, Zhizhou and Liu, Terry and Yu, Yang and Zhang, Chongjie},
	urldate = {2025-06-23},
	date = {2021-10-04},
	eprinttype = {arxiv},
	eprint = {2008.01062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{rashid_monotonic_2020,
	title = {Monotonic value function factorisation for deep multi-agent reinforcement learning},
	volume = {21},
	issn = {1532-4435},
	abstract = {In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is {QMIX}, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. {QMIX} employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of {QMIX}, we propose the {StarCraft} Multi-Agent Challenge ({SMAC}) as a new benchmark for deep multi-agent reinforcement learning. We evaluate {QMIX} on a challenging set of {SMAC} scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.},
	pages = {178:7234--178:7284},
	number = {1},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
	date = {2020-01-01},
}

@article{shavandi_multi-agent_2022,
	title = {A multi-agent deep reinforcement learning framework for algorithmic trading in financial markets},
	volume = {208},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013082},
	doi = {10.1016/j.eswa.2022.118124},
	abstract = {Algorithmic trading based on machine learning is a developing and promising field of research. Financial markets have a complex, uncertain, and dynamic nature, making them challenging for trading. Some financial theories, such as the fractal market hypothesis, believe that the markets behave based on the collective psychology of investors who trade with different investment horizons and interpretations of information. Accordingly, a multi-agent deep reinforcement learning framework is proposed in this paper to trade on the collective intelligence of multiple agents, each of which is an expert trader on a specific timeframe. The proposed framework works in a hierarchical structure in which the flow of knowledge is from the agents trading at higher timeframes to the agents trading at lower timeframes, making them highly robust to the noise in financial time series. The Deep Q-learning algorithm is utilized for training the agents in the framework. The performance of the proposed framework is evaluated through extensive numerical experiments conducted on a historical dataset of the {EUR}/{USD} currency pair. The results demonstrate that the proposed multi-agent framework, based on several return-based and risk-based performance measures, outperforms single independent agents and several benchmark trading strategies in all investigated trading timeframes. The robust performance of the multi-agent framework throughout the trading period makes it suitable for algorithmic trading in financial markets.},
	pages = {118124},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Shavandi, Ali and Khedmati, Majid},
	urldate = {2025-06-23},
	date = {2022-12-01},
	keywords = {Algorithmic trading, Deep Q-learning, Multi-agent, Multi-timeframe, Reinforcement learning},
}

@misc{zhang_multi-agent_2025,
	title = {Multi-Agent Reinforcement Learning in Wireless Distributed Networks for 6G},
	url = {http://arxiv.org/abs/2502.05812},
	doi = {10.48550/arXiv.2502.05812},
	abstract = {The introduction of intelligent interconnectivity between the physical and human worlds has attracted great attention for future sixth-generation (6G) networks, emphasizing massive capacity, ultra-low latency, and unparalleled reliability. Wireless distributed networks and multi-agent reinforcement learning ({MARL}), both of which have evolved from centralized paradigms, are two promising solutions for the great attention. Given their distinct capabilities, such as decentralization and collaborative mechanisms, integrating these two paradigms holds great promise for unleashing the full power of 6G, attracting significant research and development attention. This paper provides a comprehensive study on {MARL}-assisted wireless distributed networks for 6G. In particular, we introduce the basic mathematical background and evolution of wireless distributed networks and {MARL}, as well as demonstrate their interrelationships. Subsequently, we analyze different structures of wireless distributed networks from the perspectives of homogeneous and heterogeneous. Furthermore, we introduce the basic concepts of {MARL} and discuss two typical categories, including model-based and model-free. We then present critical challenges faced by {MARL}-assisted wireless distributed networks, providing important guidance and insights for actual implementation. We also explore an interplay between {MARL}-assisted wireless distributed networks and emerging techniques, such as information bottleneck and mirror learning, delivering in-depth analyses and application scenarios. Finally, we outline several compelling research directions for future {MARL}-assisted wireless distributed networks.},
	number = {{arXiv}:2502.05812},
	publisher = {{arXiv}},
	author = {Zhang, Jiayi and Liu, Ziheng and Zhu, Yiyang and Shi, Enyu and Xu, Bokai and Yuen, Chau and Niyato, Dusit and Debbah, Mérouane and Jin, Shi and Ai, Bo and Xuemin and Shen},
	urldate = {2025-06-23},
	date = {2025-02-09},
	eprinttype = {arxiv},
	eprint = {2502.05812 [cs]},
	keywords = {Computer Science - Information Theory, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Information Theory},
}

@article{belgacem_intelligent_2022,
	title = {Intelligent multi-agent reinforcement learning model for resources allocation in cloud computing},
	volume = {34},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157822001008},
	doi = {10.1016/j.jksuci.2022.03.016},
	abstract = {Now more than ever, optimizing resource allocation in cloud computing is becoming more critical due to the growth of cloud computing consumers and meeting the computing demands of modern technology. Cloud infrastructures typically consist of heterogeneous servers, hosting multiple virtual machines with potentially different specifications, and volatile resource usage. This makes the resource allocation face many issues such as energy conservation, fault tolerance, workload balancing, etc. Finding a comprehensive solution that considers all these issues is one of the essential concerns of cloud service providers. This paper presents a new resource allocation model based on an intelligent multi-agent system and reinforcement learning method ({IMARM}). It combines the multi-agent characteristics and the Q-learning process to improve the performance of cloud resource allocation. {IMARM} uses the properties of multi-agent systems to dynamically allocate and release resources, thus responding well to changing consumer demands. Meanwhile, the reinforcement learning policy makes virtual machines move to the best state according to the current state environment. Also, we study the impact of {IMARM} on execution time. The experimental results showed that our proposed solution performs better than other comparable algorithms regarding energy consumption and fault tolerance, with reasonable load balancing and respectful execution time.},
	pages = {2391--2404},
	number = {6},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Belgacem, Ali and Mahmoudi, Saïd and Kihl, Maria},
	urldate = {2025-06-23},
	date = {2022-06-01},
	keywords = {Cloud computing, Energy consumption, Fault tolerance, Load balancing, Multi-agent system, Q-learning, Resource allocation},
}

@misc{bansal_emergent_2018,
	title = {Emergent Complexity via Multi-Agent Competition},
	url = {http://arxiv.org/abs/1710.03748},
	doi = {10.48550/arXiv.1710.03748},
	abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/{eR}7fbX},
	number = {{arXiv}:1710.03748},
	publisher = {{arXiv}},
	author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
	urldate = {2025-06-23},
	date = {2018-03-14},
	eprinttype = {arxiv},
	eprint = {1710.03748 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{leibo_multi-agent_2017,
	location = {Richland, {SC}},
	title = {Multi-agent Reinforcement Learning in Sequential Social Dilemmas},
	series = {{AAMAS} '17},
	abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
	pages = {464--473},
	booktitle = {Proceedings of the 16th Conference on Autonomous Agents and {MultiAgent} Systems},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
	urldate = {2025-06-23},
	date = {2017},
}

@inproceedings{lowe_multi-agent_2017,
	location = {Red Hook, {NY}, {USA}},
	title = {Multi-agent actor-critic for mixed cooperative-competitive environments},
	isbn = {978-1-5108-6096-4},
	series = {{NIPS}'17},
	abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
	pages = {6382--6393},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
	urldate = {2025-06-23},
	date = {2017},
}
